{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM2jn3UQEUnNVcPw5Z0gJK/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SamuelWelz/MachineLearning/blob/main/ML_Assignment_RAG_System%2BAgentensystem.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Installation der benötigten Pakete"
      ],
      "metadata": {
        "id": "EzjsVRkmlfaK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Installation der benötigten Python-Pakete in Google Colab.\n",
        "# Das Ausrufezeichen (!) führt Terminalbefehle in Colab aus, hier z. B. pip zur Installation.\n",
        "\n",
        "# LangChain-Module für die Verwendung von OpenAI-LLMs und Community-Tools:\n",
        "# - `langchain-openai`: Stellt einfache Python-Klassen bereit, um OpenAI-Modelle (z. B. GPT-4o)\n",
        "#    anzusprechen. Die komplexe HTTP-Kommunikation mit der OpenAI-API (Authentifizierung, Request-Format,\n",
        "#    Response-Parsing) wird vollständig abstrahiert – wir müssen nur noch .invoke() aufrufen.\n",
        "# - `langchain-community`: Enthält viele Dokumenten-Loader (z. B. für PDFs, Webseiten, CSVs)\n",
        "#    und Integrationen mit Vektordatenbanken wie FAISS, Pinecone etc.\n",
        "!pip install langchain-openai langchain-community\n",
        "\n",
        "# `pypdf`: Eine Hilfsbibliothek zur Verarbeitung von PDF-Dateien.\n",
        "# Wird von LangChain intern verwendet, um PDFs seitenweise zu lesen.\n",
        "# Ohne diese Bibliothek wäre ein manuelles Parsen des PDF-Inhalts nötig.\n",
        "!pip install pypdf\n",
        "\n",
        "# `faiss-cpu`: Installiert die CPU-Version der Vektor-Datenbank FAISS von Meta AI.\n",
        "# Damit kann man lokal Vektoren (z. B. Embeddings) effizient speichern und durchsuchen.\n",
        "# LangChain abstrahiert die native C++-FAISS-Schnittstelle vollständig,\n",
        "# sodass man mit Python-Objekten wie `FAISS.from_documents(...)` arbeiten kann.\n",
        "!pip install faiss-cpu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "6rYmD3yYleJJ",
        "outputId": "b706f0fa-ca71-4a92-9f4f-0bfe3684393c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain-openai\n",
            "  Downloading langchain_openai-0.3.28-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting langchain-community\n",
            "  Downloading langchain_community-0.3.27-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.68 in /usr/local/lib/python3.11/dist-packages (from langchain-openai) (0.3.72)\n",
            "Requirement already satisfied: openai<2.0.0,>=1.86.0 in /usr/local/lib/python3.11/dist-packages (from langchain-openai) (1.98.0)\n",
            "Requirement already satisfied: tiktoken<1,>=0.7 in /usr/local/lib/python3.11/dist-packages (from langchain-openai) (0.9.0)\n",
            "Requirement already satisfied: langchain<1.0.0,>=0.3.26 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.27)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.42)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (3.12.15)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (8.5.0)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community)\n",
            "  Downloading pydantic_settings-2.10.1-py3-none-any.whl.metadata (3.4 kB)\n",
            "Requirement already satisfied: langsmith>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.4.10)\n",
            "Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain-community)\n",
            "  Downloading httpx_sse-0.4.1-py3-none-any.whl.metadata (9.4 kB)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.20.1)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.26->langchain-community) (0.3.9)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.26->langchain-community) (2.11.7)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.68->langchain-openai) (1.33)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.68->langchain-openai) (4.14.1)\n",
            "Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.68->langchain-openai) (25.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain-community) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain-community) (3.11.1)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain-community) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain-community) (0.23.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.86.0->langchain-openai) (4.10.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.86.0->langchain-openai) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.86.0->langchain-openai) (0.10.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.86.0->langchain-openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.86.0->langchain-openai) (4.67.1)\n",
            "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain-community)\n",
            "  Downloading python_dotenv-1.1.1-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (2025.8.3)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.2.3)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken<1,>=0.7->langchain-openai) (2024.11.6)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.68->langchain-openai) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.26->langchain-community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.26->langchain-community) (2.33.2)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Downloading langchain_openai-0.3.28-py3-none-any.whl (70 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.6/70.6 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_community-0.3.27-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m34.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading httpx_sse-0.4.1-py3-none-any.whl (8.1 kB)\n",
            "Downloading pydantic_settings-2.10.1-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.2/45.2 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.1.1-py3-none-any.whl (20 kB)\n",
            "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
            "Installing collected packages: python-dotenv, mypy-extensions, marshmallow, httpx-sse, typing-inspect, pydantic-settings, dataclasses-json, langchain-openai, langchain-community\n",
            "Successfully installed dataclasses-json-0.6.7 httpx-sse-0.4.1 langchain-community-0.3.27 langchain-openai-0.3.28 marshmallow-3.26.1 mypy-extensions-1.1.0 pydantic-settings-2.10.1 python-dotenv-1.1.1 typing-inspect-0.9.0\n",
            "Collecting pypdf\n",
            "  Downloading pypdf-5.9.0-py3-none-any.whl.metadata (7.1 kB)\n",
            "Downloading pypdf-5.9.0-py3-none-any.whl (313 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m313.2/313.2 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pypdf\n",
            "Successfully installed pypdf-5.9.0\n",
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.11.0.post1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (25.0)\n",
            "Downloading faiss_cpu-1.11.0.post1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (31.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.3/31.3 MB\u001b[0m \u001b[31m58.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: faiss-cpu\n",
            "Successfully installed faiss-cpu-1.11.0.post1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Alle wichtigen Importe"
      ],
      "metadata": {
        "id": "I5WrDeA4k74e"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "xw-kPu25lShy"
      },
      "outputs": [],
      "source": [
        "# Zugriff auf Secrets in Google Colab (API-Key)\n",
        "# ----------------------------------------------\n",
        "# `userdata.get(\"apikey_openai\")` greift sicher auf einen in Colab hinterlegten geheimen API-Key zu.\n",
        "# Vorteil: Der API-Key ist nicht im Code sichtbar (Datenschutz!), sondern wurde vorher manuell unter dem Schlüssel-Icon links in Colab eingetragen.\n",
        "from google.colab import userdata\n",
        "OPENAI_API_KEY = userdata.get(\"apikey_openai\")\n",
        "\n",
        "# Zugriff auf OpenAI-Modelle (LLM)\n",
        "# --------------------------------\n",
        "# `ChatOpenAI` ist eine LangChain-Klasse, die das Senden von Prompts an OpenAI-Modelle stark vereinfacht.\n",
        "# → Wegabstrahiert werden:\n",
        "#   - die direkte HTTP-Kommunikation mit der OpenAI-API\n",
        "#   - das Serialisieren des Prompts\n",
        "#   - das Parsen der JSON-Antwort\n",
        "#   - Fehlerbehandlung (Rate Limits etc.)\n",
        "#   - das Token-Management\n",
        "# Der Entwickler arbeitet nur noch mit einer einfachen Python-Klasse und ruft `.invoke()` auf.\n",
        "\n",
        "from langchain_openai.chat_models import ChatOpenAI\n",
        "\n",
        "# Zugriff auf OpenAI-Embeddings\n",
        "# -----------------------------\n",
        "# `OpenAIEmbeddings` erzeugt Vektorrepräsentationen (sog. Embeddings) für Texte.\n",
        "# → Wegabstrahiert:\n",
        "#   - API-Kommunikation mit OpenAI (Endpoint für Embeddings)\n",
        "#   - Formatierung der Eingabetexte\n",
        "#   - Rückgabe der Embedding-Vektoren (Listen aus Floats)\n",
        "#   - Wiederholung bei API-Fehlern oder zu langen Texten\n",
        "# Diese Embeddings sind zentral für das spätere Retrieval (Vektorsuche).\n",
        "from langchain_openai.embeddings import OpenAIEmbeddings\n",
        "\n",
        "# PDF-Dateien laden und in lesbaren Text umwandeln\n",
        "# ------------------------------------------------\n",
        "# `PyPDFLoader` ist ein Document Loader aus LangChain Community.\n",
        "# Er kapselt:\n",
        "#   - das Parsen der PDF-Datei (basierend auf `pypdf`)\n",
        "#   - das Extrahieren der Texte aus jeder Seite\n",
        "#   - das Umwandeln der Seiten in `Document`-Objekte von LangChain\n",
        "# Ohne diese Abstraktion müsste man sich mit `pypdf`, Textextraktion, Fehlern etc. selbst beschäftigen.\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "\n",
        "# Text in sinnvolle Abschnitte (Chunks) teilen\n",
        "# --------------------------------------------\n",
        "# `RecursiveCharacterTextSplitter` ist ein Utility, das Text in semantisch sinnvolle Stücke aufteilt,\n",
        "# wobei z. B. auf Satz- oder Absatzgrenzen geachtet wird.\n",
        "# → Wegabstrahiert:\n",
        "#   - eigene Logik zum Splitten langer Texte in passende Häppchen\n",
        "#   - Verwaltung von Overlap zwischen Chunks (z. B. 100 Token Overlap)\n",
        "#   - Handling von Grenzen bei Wörtern, Absätzen etc.\n",
        "# Dies ist entscheidend für sinnvolle Embedding-Vektoren und präzise semantische Suche.\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "# FAISS als Vektor-Datenbank\n",
        "# --------------------------\n",
        "# `FAISS` ist ein schneller In-Memory-Vectorstore für Ähnlichkeitssuchen.\n",
        "# Die LangChain-Integration `langchain_community.vectorstores.FAISS` kapselt:\n",
        "#   - die Generierung von Vektoren für Dokument-Chunks\n",
        "#   - das Indexieren der Vektoren\n",
        "#   - die Suche nach den ähnlichsten Chunks für eine Query\n",
        "#   - die Rückgabe relevanter Dokumente anhand der Embedding-Ähnlichkeit\n",
        "# Dadurch entfällt das manuelle Arbeiten mit Vektor-Matrizen und KNN-Algorithmen.\n",
        "from langchain_community.vectorstores import FAISS\n",
        "\n",
        "# Prompt-Vorlagen (Templates) erstellen\n",
        "# -------------------------------------\n",
        "# `ChatPromptTemplate` erlaubt es, strukturierte Prompts mit Platzhaltern zu erstellen.\n",
        "# Die Abstraktion übernimmt:\n",
        "#   - das Einsetzen von Variablen wie {context}, {question}\n",
        "#   - das sichere Formatieren ohne Zeichenprobleme\n",
        "#   - das Vorbereiten des finalen Prompts zur Übergabe ans LLM\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "# LangChain Runnable: Pipe-Element, das einfach Daten durchreicht\n",
        "# ----------------------------------------------------------------\n",
        "# `RunnablePassthrough()` wird später in der Chain genutzt, um eine Frage unverändert durchzuleiten.\n",
        "# Es ist ein Baustein in der modularen LangChain-Architektur (jede Komponente ist eine „runnable pipeline unit“).\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "\n",
        "# Output-Parser\n",
        "# -------------\n",
        "# `StrOutputParser` wandelt das Roh-Ergebnis eines LLM-Aufrufs (vom Typ AIMessage) in einen reinen String um.\n",
        "# Abstraktion:\n",
        "#   - entfernt technische Metadaten aus der Antwort\n",
        "#   - sorgt dafür, dass am Ende ein sauberer Antwort-Text steht, mit dem man weiterarbeiten kann\n",
        "from langchain_core.output_parsers import StrOutputParser"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. PDF laden"
      ],
      "metadata": {
        "id": "JQ01l0kbmerf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# PDF-Datei aus GitHub laden\n",
        "# ----------------------------------------------------------\n",
        "# Der bisherige Ansatz mit `pdf_path` funktioniert nur für lokale Dateien im Colab-Dateisystem.\n",
        "# Bei einem GitHub-Link (z. B. https://github.com/...) wird jedoch nicht direkt die PDF,\n",
        "# sondern eine HTML-Webseite geladen, die die PDF nur anzeigt.\n",
        "# Für die Verarbeitung wird der direkte RAW-Content der Datei benötigt.\n",
        "\n",
        "import requests\n",
        "import tempfile\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "\n",
        "# URL zum PDF im GitHub-Repository (RAW-Dateiversion verwenden!)\n",
        "# Der Permalink aus GitHub muss in den \"raw.githubusercontent.com\"-Link umgewandelt werden.\n",
        "pdf_url = \"https://raw.githubusercontent.com/SamuelWelz/MachineLearning/a68810309c535ece8c4e8bb6ed95d392321323c7/BitcoinMining.pdf\"\n",
        "\n",
        "# PDF von GitHub herunterladen\n",
        "response = requests.get(pdf_url)\n",
        "response.raise_for_status()  # Falls Download fehlschlägt, wird ein HTTPError ausgelöst.\n",
        "\n",
        "# Temporäre Datei anlegen und PDF-Inhalt dort speichern\n",
        "# Hintergrund: PyPDFLoader benötigt einen Dateipfad, nicht direkt die Binärdaten.\n",
        "with tempfile.NamedTemporaryFile(delete=False, suffix=\".pdf\") as tmp_file:\n",
        "    tmp_file.write(response.content)\n",
        "    temp_pdf_path = tmp_file.name  # Pfad zur temporären PDF-Datei\n",
        "\n",
        "# PDF laden mit dem LangChain PDF-Loader\n",
        "# --------------------------------------\n",
        "# PyPDFLoader ist ein DocumentLoader aus der LangChain Community-Bibliothek.\n",
        "# Er abstrahiert intern:\n",
        "#   1. Öffnen der PDF-Datei über pypdf\n",
        "#   2. Extraktion des Textinhalts aus jeder einzelnen Seite\n",
        "#   3. Umwandlung in eine Liste von Document-Objekten (ein Objekt pro Seite),\n",
        "#      wobei jedes Objekt `page_content` (Text) und `metadata` (z. B. Seitenzahl) enthält.\n",
        "loader = PyPDFLoader(temp_pdf_path)\n",
        "\n",
        "# `.load()` führt den eigentlichen Lese- und Extraktionsprozess aus.\n",
        "# Ergebnis: Liste von Document-Objekten, die als Eingabe für Chunking/Embedding genutzt werden können.\n",
        "documents = loader.load()"
      ],
      "metadata": {
        "id": "-B_sGVPul_Le"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Dokument splitten in Chunks"
      ],
      "metadata": {
        "id": "P29UV34ImgH_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dokument in gleichgroße Text-Chunks aufteilen\n",
        "# ---------------------------------------------\n",
        "# LangChain erwartet für Embedding- und Retrieval-Anwendungen kleinere,\n",
        "# sinnvoll segmentierte Abschnitte, sogenannte \"Chunks\".\n",
        "# Große Dokumente (wie lange PDF-Seiten) müssen daher in kleinere Stücke\n",
        "# unterteilt werden, damit:\n",
        "# - das Token-Limit von LLMs nicht überschritten wird\n",
        "# - relevante Informationen besser extrahiert werden können\n",
        "\n",
        "# Der RecursiveCharacterTextSplitter übernimmt das automatische \"Chunking\".\n",
        "# Dabei passiert im Hintergrund folgendes:\n",
        "# 1. Das Dokument wird rekursiv entlang vordefinierter Trennzeichen (Absatz, Satz, Wort) geteilt.\n",
        "#    Falls der Text zu groß ist, wird auf feinere Trennmethoden zurückgegriffen.\n",
        "# 2. Der `chunk_size` legt die Zielgröße der Chunks in Zeichen fest (hier: 250).\n",
        "# 3. Der `chunk_overlap` sorgt dafür, dass sich die Chunks überlappen (hier: 75 Zeichen).\n",
        "#    → Dadurch bleiben zusammenhängende Gedanken auch bei Chunk-Grenzen erhalten\n",
        "#      und Kontext wird robuster vermittelt (wichtig für Embedding & Retrieval).\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=250,       # Jeder Chunk enthält max. 250 Zeichen\n",
        "    chunk_overlap=75      # Davon überlappen sich 75 Zeichen mit dem nächsten Chunk\n",
        ")\n",
        "\n",
        "# Die Methode `.split_documents()` verarbeitet die LangChain-Document-Objekte\n",
        "# und erzeugt daraus eine Liste von kleineren Dokumenten-Chunks\n",
        "# → Jeder Eintrag in `split_documents` ist ein eigenständiger `Document` mit eigenem Textabschnitt\n",
        "\n",
        "split_documents = text_splitter.split_documents(documents)"
      ],
      "metadata": {
        "id": "2568VERRmizA"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Embeddings erzeugen und in Vectorstore speichern"
      ],
      "metadata": {
        "id": "FRIZVGvCmpqX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Embedding-Modell definieren\n",
        "# ----------------------------\n",
        "# Definiert ein Embedding-Modell von OpenAI, das dazu verwendet wird,\n",
        "# um Text-Chunks in semantische Vektor-Repräsentationen zu konvertieren.\n",
        "#\n",
        "# Eingesetztes Modell: \"text-embedding-3-small\"\n",
        "# - Liefert numerische Vektoren, die semantische Ähnlichkeit zwischen Texten abbilden\n",
        "# - Eignet sich für effiziente Ähnlichkeitssuche im Retrieval-Schritt\n",
        "#\n",
        "# Abstrahierung durch LangChain:\n",
        "# - Automatischer API-Aufruf zur OpenAI-Schnittstelle\n",
        "# - Automatische Tokenisierung des Textes und Rückgabe der Embeddings als Listen von Float-Werten\n",
        "# - Kein manuelles Handling der HTTP-Anfragen oder JSON-Parsing erforderlich\n",
        "\n",
        "embeddings = OpenAIEmbeddings(\n",
        "    model=\"text-embedding-3-small\",\n",
        "    openai_api_key=OPENAI_API_KEY\n",
        ")\n",
        "\n",
        "\n",
        "# Vectorstore erstellen (FAISS lokal im Arbeitsspeicher)\n",
        "# -------------------------------------------------------\n",
        "# Erstellt einen Vektorindex aus den zuvor erzeugten Text-Chunks und den zugehörigen Embeddings.\n",
        "# Verwendet wird FAISS (Facebook AI Similarity Search), ein Framework für effiziente\n",
        "# Ähnlichkeitssuche in hochdimensionalen Vektorräumen.\n",
        "#\n",
        "# Hintergrundprozesse:\n",
        "# - Jeder Text-Chunk wird vom Embedding-Modell in einen Vektor umgewandelt\n",
        "# - Die resultierenden Vektoren werden in FAISS gespeichert und indiziert\n",
        "# - Der Index ermöglicht spätere semantische Suchen über `similarity_search()`\n",
        "#\n",
        "# Vorteile durch LangChain:\n",
        "# - Kein manueller Aufbau des FAISS-Index notwendig\n",
        "# - Speicherung der Dokumente und Vektoren erfolgt transparent\n",
        "# - Zugriff auf Inhalte durch Retrieval-Methoden mit automatischer Relevanzbewertung\n",
        "\n",
        "vectorstore = FAISS.from_documents(split_documents, embedding=embeddings)\n"
      ],
      "metadata": {
        "id": "dH3D_Dkpmnb9"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Prompt-Vorlage definieren"
      ],
      "metadata": {
        "id": "5ZfyRdMnnHE0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "# Prompt-Template definieren\n",
        "# ---------------------------\n",
        "# Erstellt eine strukturierte Prompt-Vorlage für das LLM, die aus zwei Komponenten besteht:\n",
        "# 1. Kontext (aus der Vektordatenbank)\n",
        "# 2. Frage (vom Nutzer gestellt)\n",
        "#\n",
        "# Der Prompt stellt sicher, dass das Modell die Frage auf Basis des Kontextes beantwortet\n",
        "# und im Zweifelsfall eine standardisierte Aussage trifft (\"Ich weiß es nicht\").\n",
        "\n",
        "template = \"\"\"\n",
        "Beantworte die Frage basierend auf dem gegebenen Kontext.\n",
        "Wenn du keine Antwort findest, sag bitte: \"Ich weiß es nicht.\"\n",
        "\n",
        "Context: {context}\n",
        "\n",
        "Question: {question}\n",
        "\"\"\"\n",
        "\n",
        "# Prompt-Objekt erstellen\n",
        "# ------------------------\n",
        "# Die Methode `from_template()` abstrahiert mehrere manuelle Schritte:\n",
        "# - Ersetzt Platzhalter {context} und {question} automatisch zur Laufzeit\n",
        "# - Verpackt den Prompt als standardisiertes LangChain-Objekt\n",
        "# - Unterstützt spätere Komposition in Chains (z. B. | Prompt | Modell)\n",
        "#\n",
        "# Vorteil:\n",
        "# - Kein manuelles Prompt-Building mit String-Konkatenation notwendig\n",
        "# - Bessere Wiederverwendbarkeit und Klarheit der Prompt-Struktur\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(template)"
      ],
      "metadata": {
        "id": "ESTtYPwwnGu9"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7. OpenAI LLM & Parser vorbereiten"
      ],
      "metadata": {
        "id": "ykP13ULmna2j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Die Klasse `ChatOpenAI` kapselt den Zugriff auf die OpenAI-API und übernimmt mehrere Schritte:\n",
        "# - Aufbau und Verwaltung der HTTP-Verbindung zur OpenAI API\n",
        "# - Formatierung der Anfrage (z. B. Modellwahl, Parameter wie Temperatur und Tokenlimit)\n",
        "# - Verwaltung der Modellantwort (inkl. Token-Handling, Logging, Fehlerbehandlung)\n",
        "#\n",
        "# Vorteil:\n",
        "# - Keine manuelle API-Nutzung mit HTTP-Requests oder JSON erforderlich\n",
        "# - Einheitliche Schnittstelle für alle unterstützten OpenAI-Modelle\n",
        "# - Direkte Integration in LangChain-Pipelines möglich\n",
        "\n",
        "# GPT-4o-mini verwenden\n",
        "model = ChatOpenAI(\n",
        "    openai_api_key=OPENAI_API_KEY,\n",
        "    model=\"gpt-4o-mini\",\n",
        "    temperature=0.2,    # Steuert die Kreativität des Modells. Niedriger Wert führt zu deterministischeren, prägnanteren Antworten.\n",
        "    max_tokens=200      # Begrenzung der maximalen Antwortlänge, um den Tokenverbrauch zu kontrollieren und Kosten zu senken.\n",
        ")\n",
        "\n",
        "\n",
        "# Parser: Nur der reine Antworttext soll extrahiert werden\n",
        "parser = StrOutputParser()\n",
        "\n",
        "# Der `StrOutputParser` ist ein Output-Parser, der die Antwort des Modells bereinigt:\n",
        "# - Extrahiert nur den tatsächlichen Antwortinhalt aus dem vollständigen `AIMessage`-Objekt\n",
        "# - Entfernt Metadaten, Tokeninformationen oder andere technische Elemente\n",
        "#\n",
        "# Vorteil:\n",
        "# - Die Ausgabe kann direkt weiterverwendet oder angezeigt werden\n",
        "# - Kein manuelles Parsen der Rückgabe erforderlich"
      ],
      "metadata": {
        "id": "grmt8fk8nbr7"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8. Chain bauen"
      ],
      "metadata": {
        "id": "cXfBWsXSngMU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Aufbau der vollständigen Retrieval-Augmented Generation (RAG)-Pipeline\n",
        "# -------------------------------------------\n",
        "# Diese Chain kombiniert alle zuvor definierten Schritte zu einem automatisierten Ablauf:\n",
        "# 1. Die Nutzerfrage wird an den Retriever übergeben.\n",
        "# 2. Der Retriever durchsucht den Vektorstore nach passenden Textstellen (Chunks) und gibt diese als Kontext zurück.\n",
        "# 3. Kontext und Frage werden in einem Prompt-Template kombiniert.\n",
        "# 4. Der formattierte Prompt wird an das Sprachmodell (LLM) übergeben.\n",
        "# 5. Die Antwort des LLM wird durch einen Parser in reinen Text umgewandelt.\n",
        "# LangChain abstrahiert dabei viele technische Details wie Embedding-Suche, Prompt-Zusammenbau oder Modellkommunikation.\n",
        "\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "\n",
        "# Retriever-Chain aufbauen\n",
        "chain = (\n",
        "    {\n",
        "        \"context\": vectorstore.as_retriever(),       # Der Vektorstore wird in einen Retriever umgewandelt, der bei einer Anfrage relevante Chunks zurückliefert\n",
        "        \"question\": RunnablePassthrough()            # Übergibt die Frage unverändert weiter, da sie nicht verändert oder vorverarbeitet werden muss\n",
        "    }\n",
        "    | prompt                                         # Kombiniert Kontext und Frage gemäß der Prompt-Vorlage zu einem vollständigen Prompt\n",
        "    | model                                          # Sendet den formatierten Prompt an das Sprachmodell (z. B. GPT-4o-mini)\n",
        "    | parser                                         # Filtert aus der Modellantwort den reinen Antworttext heraus\n",
        ")\n"
      ],
      "metadata": {
        "id": "Uy7uSjcCnfzV"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 9. Testlauf der Pipeline"
      ],
      "metadata": {
        "id": "QGLRN0sFnk-6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Definition einer Beispiel-Frage, die an das RAG-System gestellt wird.\n",
        "frage = \"Was haben Gewächshäuser mit BTC-Minig zu tun?\"\n",
        "\n",
        "# Weitere mögliche Fragen (zur Demonstration/Tests):\n",
        "# - Was gibt es für Arten von BTC-Mining?\n",
        "# - Was ist Bitcoin-Mining?\n",
        "\n",
        "# Die gesamte Chain wird durch .invoke(frage) ausgeführt:\n",
        "# Intern passiert dabei Folgendes:\n",
        "# 1. Die Frage wird an den Retriever übergeben.\n",
        "# 2. Der Retriever berechnet ein Embedding der Frage.\n",
        "# 3. Dieses Embedding wird mit den Vektoren im Vektorstore verglichen\n",
        "# 4. Die ähnlichsten Chunks (Kontext) werden selektiert und gemeinsam mit der Frage an das Prompt-Template übergeben.\n",
        "# 5. Das resultierende Prompt wird an das OpenAI-Modell gesendet.\n",
        "# 6. Das Modell erzeugt eine Antwort.\n",
        "# 7. Der Output-Parser entfernt technische Metadaten und gibt den reinen Antworttext zurück.\n",
        "\n",
        "antwort = chain.invoke(frage)\n",
        "\n",
        "# Ausgabe der modellgenerierten Antwort zur Kontrolle\n",
        "print(\"Antwort:\\n\", antwort)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oAtLahj0nl5s",
        "outputId": "deb19d25-21f0-4450-a2f4-d5e16b3b9a9d"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Antwort:\n",
            " Gewächshäuser können mit BTC-Mining in Verbindung stehen, indem die Wärme, die von Mining-Servern erzeugt wird, genutzt wird, um das Gewächshaus zu heizen. Dadurch entstehen \"echte Bitcoin-Blumen\", und die Energie für die Mining-Hardware kann aus Solarzellen stammen, die auf dem Dach des Gewächshauses installiert sind. Dies schafft einen umweltfreundlichen Mining-Prozess, der die Energieerzeugung und -weitergabe effizient nutzt.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# --------------------- AGENTENSYSTEM ---------------------"
      ],
      "metadata": {
        "id": "0xKg5UaLNsyR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports & Guards"
      ],
      "metadata": {
        "id": "zkS58kQNRtxC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importiert Standardtypen für Typannotationen von Listen\n",
        "from typing import List\n",
        "\n",
        "# Importiert Pydantic-Basisklasse und Field, um Datenmodelle mit Validierung zu erstellen\n",
        "from pydantic import BaseModel, Field\n",
        "\n",
        "# Versucht zuerst den aktuellen LangChain-OpenAI-Import, fällt bei älteren Versionen auf den alten Pfad zurück\n",
        "try:\n",
        "    from langchain_openai import ChatOpenAI\n",
        "except Exception:\n",
        "    from langchain_openai.chat_models import ChatOpenAI  # Fallback für ältere Versionen\n",
        "\n",
        "# Importiert Hilfsklasse zum Erstellen strukturierter Chat-Prompt-Vorlagen mit Platzhaltern\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "# Importiert den Tool-Dekorator, um Python-Funktionen als Werkzeuge für Agents nutzbar zu machen\n",
        "from langchain.tools import tool\n",
        "\n",
        "# Importiert Klassen, um Agents mit Tools zu bauen und auszuführen\n",
        "from langchain.agents import AgentExecutor, create_openai_tools_agent\n",
        "\n",
        "# Stellt sicher, dass der API-Key und der Vektorindex (vectorstore) bereits aus dem RAG-Setup existieren\n",
        "assert 'OPENAI_API_KEY' in globals() and OPENAI_API_KEY, \"OPENAI_API_KEY fehlt. Bitte RAG-Setup-Zellen ausführen.\"\n",
        "assert 'vectorstore' in globals(), \"vectorstore fehlt. Bitte RAG-Setup-Zellen ausführen (FAISS-Index).\"\n",
        "\n",
        "# Erstellt das Basismodell für die OpenAI-Chat-API mit geringer Temperatur für deterministische Antworten\n",
        "# max_tokens wird hier bewusst nicht gesetzt, um eine spätere flexible Anpassung zu erlauben\n",
        "base_model = ChatOpenAI(\n",
        "    openai_api_key=OPENAI_API_KEY,\n",
        "    model=\"gpt-4o-mini\",\n",
        "    temperature=0.2,\n",
        ")\n",
        "\n",
        "# Bindet ein hartes Completion-Limit an das Modell, damit alle Aufrufe max. 500 Tokens zurückgeben\n",
        "model = base_model.bind(max_completion_tokens=500)\n",
        "\n",
        "# Konsolenausgabe, um zu bestätigen, dass Modell und Vectorstore bereitstehen\n",
        "print(\"OK – model und vectorstore verfügbar.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PrskhZxwRAtn",
        "outputId": "83d0004b-c2cf-4834-a0ff-1dde09c9f704"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OK – model und vectorstore verfügbar.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tools (RAG + Calculator)"
      ],
      "metadata": {
        "id": "YodZZhMnRoua"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== 1) TOOLS =====\n",
        "# In diesem Abschnitt werden zwei eigene Tools definiert, die später vom Agenten\n",
        "# automatisch aufgerufen werden können.\n",
        "# Tools sind in LangChain gekapselte Python-Funktionen, die mit zusätzlichen\n",
        "# Metadaten (Name, Beschreibung) versehen werden.\n",
        "# Sie ermöglichen es einem Agenten, während der Laufzeit gezielt externe Funktionen\n",
        "# zu nutzen\n",
        "#\n",
        "# Tool 1: \"rag_search\"\n",
        "# --------------------\n",
        "# Zweck:\n",
        "# - Führt eine semantische Suche im FAISS-Vectorstore durch\n",
        "# - Gibt die Top-k relevantesten Text-Snippets aus dem Dokument zurück\n",
        "#\n",
        "# Ablauf:\n",
        "# 1. Nimmt eine Suchanfrage (query) entgegen\n",
        "# 2. Führt mit `vectorstore.similarity_search()` eine Vektorsuche aus\n",
        "# 3. Kürzt die Ergebnisse mit einer internen Hilfsfunktion `_short()`, um die Tokenanzahl\n",
        "#    zu reduzieren (max. 350 Zeichen pro Treffer)\n",
        "# 4. Baut eine strukturierte Ergebnisliste auf (Nummer, Seitenzahl, Quelle, Text)\n",
        "# 5. Gibt alles als String zurück\n",
        "#\n",
        "# Vorteil:\n",
        "# - Der Agent muss nicht wissen, wie FAISS funktioniert oder wie die Suche implementiert ist.\n",
        "# - Die gesamte Logik (Suche, Kürzen, Formatieren) ist gekapselt und wiederverwendbar.\n",
        "\n",
        "@tool(\"rag_search\")\n",
        "def rag_search(query: str, k: int = 2) -> str:  # war k=4\n",
        "    \"\"\"Durchsucht den FAISS-Index und liefert kompakte Top-k Snippets.\"\"\"\n",
        "    docs = vectorstore.similarity_search(query, k=k)\n",
        "\n",
        "    def _short(text, n=350):  # Kürzt lange Snippets\n",
        "        t = (text or \"\").strip().replace(\"\\n\", \" \")\n",
        "        return (t[:n] + \"…\") if len(t) > n else t\n",
        "\n",
        "    lines = []\n",
        "    for i, d in enumerate(docs, start=1):\n",
        "        meta = d.metadata if isinstance(d.metadata, dict) else {}\n",
        "        src = meta.get(\"source\", \"PDF\")\n",
        "        page = meta.get(\"page\", \"?\")\n",
        "        lines.append(f\"[{i}] (S.{page}, {src}) {_short(d.page_content)}\")\n",
        "\n",
        "    return \"\\n\".join(lines)\n",
        "\n",
        "\n",
        "# Tool 2: \"calc\"\n",
        "# --------------\n",
        "# Zweck:\n",
        "# - Führt sichere mathematische Berechnungen mit Python durch, ohne `eval` zu verwenden\n",
        "#\n",
        "# Ablauf:\n",
        "# 1. Parst den Ausdruck mit `ast.parse()` in einen abstrakten Syntaxbaum (AST)\n",
        "# 2. Erlaubt nur definierte Operatoren (+, -, *, /, Potenz, Vorzeichen)\n",
        "# 3. Bewertet den Ausdruck rekursiv mit `_eval()`\n",
        "# 4. Gibt das Ergebnis als String zurück oder im Fehlerfall eine Meldung\n",
        "#\n",
        "# Vorteil:\n",
        "# - Sicherheit: Kein direkter Code-Execution-Risiko wie bei `eval`\n",
        "# - Komplette Kapselung der Parsing- und Berechnungslogik in einer einfachen Funktion\n",
        "#\n",
        "# Am Ende werden beide Tools in einer Liste `tools` gesammelt,\n",
        "# sodass sie später beim Erstellen des Agenten registriert werden können.\n",
        "\n",
        "@tool(\"calc\")\n",
        "def calc(expression: str) -> str:\n",
        "    \"\"\"Einfacher Rechner für + - * / ** und Klammern. Beispiel: '2*(3+4)/5'.\"\"\"\n",
        "    import ast, operator as op\n",
        "\n",
        "    allowed_ops = {\n",
        "        ast.Add: op.add, ast.Sub: op.sub, ast.Mult: op.mul,\n",
        "        ast.Div: op.truediv, ast.Pow: op.pow, ast.USub: op.neg\n",
        "    }\n",
        "\n",
        "    def _eval(node):\n",
        "        if isinstance(node, ast.Num): return node.n\n",
        "        if isinstance(node, ast.BinOp): return allowed_ops[type(node.op)](_eval(node.left), _eval(node.right))\n",
        "        if isinstance(node, ast.UnaryOp): return allowed_ops[type(node.op)](_eval(node.operand))\n",
        "        if isinstance(node, ast.Expression): return _eval(node.body)\n",
        "        raise ValueError(\"Nicht erlaubter Ausdruck\")\n",
        "\n",
        "    try:\n",
        "        node = ast.parse(expression, mode=\"eval\")\n",
        "        return str(_eval(node))\n",
        "    except Exception as e:\n",
        "        return f\"Fehler: {e}\"\n",
        "\n",
        "\n",
        "tools = [rag_search, calc]\n",
        "print(\"OK – Tools registriert:\", [t.name for t in tools])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JfzSWhK7NsWZ",
        "outputId": "d4762430-dc52-4840-a0fc-ddb43eea3174"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OK – Tools registriert: ['rag_search', 'calc']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Planner (Structured Output, Schritt 1 im Chaining)"
      ],
      "metadata": {
        "id": "LTXKqWv9R1L0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== 2) PLANNER =====\n",
        "# Der Planner ist die erste Stufe in unserer Agenten-Pipeline.\n",
        "# Er hat die Aufgabe, eine unstrukturierte Nutzerfrage in einen klaren, maschinenlesbaren Plan zu übersetzen.\n",
        "# Dieser Plan wird als JSON im Format unseres Pydantic-Modells `Plan` zurückgegeben.\n",
        "#\n",
        "# 1. Pydantic-Datenmodell `Plan`\n",
        "# ------------------------------\n",
        "# Das Modell beschreibt, welche Felder der Planner füllen soll:\n",
        "# - intent: Kurzbeschreibung der Aufgabe (z. B. \"Erklärung schreiben\", \"Berechnung durchführen\")\n",
        "# - needs_retrieval: Boolean, ob für die Antwort Kontext aus der RAG-Suche benötigt wird\n",
        "# - needs_calculation: Boolean, ob eine mathematische Berechnung nötig ist\n",
        "# - queries: Liste mit Suchbegriffen für die RAG-Suche\n",
        "# - calc_expressions: Liste mit Mathe-Ausdrücken, die ggf. im calc-Tool berechnet werden sollen\n",
        "# - subtasks: Liste mit einzelnen Arbeitsschritten, in denen der Agent vorgehen soll\n",
        "#\n",
        "# Vorteil:\n",
        "# - Der nachfolgende Agent (Actor) weiß genau, welche Tools er in welcher Reihenfolge verwenden muss\n",
        "# - Die Entscheidung, ob RAG oder calc benutzt wird, wird hier zentral getroffen\n",
        "\n",
        "class Plan(BaseModel):\n",
        "    intent: str = Field(..., description=\"Kurzbeschreibung der Aufgabe\")\n",
        "    needs_retrieval: bool = Field(..., description=\"Ob Kontext aus RAG benötigt wird\")\n",
        "    needs_calculation: bool = Field(..., description=\"Ob eine Berechnung nötig ist\")\n",
        "    queries: List[str] = Field(default_factory=list, description=\"Such-Queries für RAG\")\n",
        "    calc_expressions: List[str] = Field(default_factory=list, description=\"Mathe-Ausdrücke, falls nötig\")\n",
        "    subtasks: List[str] = Field(default_factory=list, description=\"Nummerierte Arbeitsschritte\")\n",
        "\n",
        "\n",
        "# 2. Prompt-Template `planner_prompt`\n",
        "# -----------------------------------\n",
        "# - Systemrolle: \"Erzeuge einen knappen Plan als JSON\"\n",
        "# - Humanrolle: Übergibt die konkrete Nutzerfrage {question} und sagt, dass der Kontext aus der internen Bitcoin-Mining-PDF kommt\n",
        "#\n",
        "# Vorteil:\n",
        "# - Wir trennen hier klar die Rollen (system vs. human), was die Modellsteuerung präziser macht\n",
        "# - Das Modell wird gezwungen, nur JSON im `Plan`-Format zu liefern (durch späteren structured output)\n",
        "\n",
        "planner_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"Erzeuge einen knappen Plan als JSON (so kurz wie möglich).\"),\n",
        "    (\"human\", \"Aufgabe: {question}\\nKontext: interne PDF zu Bitcoin Mining.\")\n",
        "])\n",
        "\n",
        "\n",
        "# 3. Verknüpfung Prompt → Modell (`planner_chain`)\n",
        "# -----------------------------------------------\n",
        "# - Wir verbinden das Prompt-Template mit unserem OpenAI-Modell (`model`)\n",
        "# - Mit `.with_structured_output(Plan)` erzwingen wir, dass die Antwort direkt als Pydantic-Objekt geparst wird\n",
        "# - Dadurch entfällt das manuelle JSON-Parsing\n",
        "#\n",
        "# Vorteil:\n",
        "# - Automatisches Validieren der Felder (Pydantic)\n",
        "# - Weniger Fehler bei der Weiterverarbeitung, weil die Struktur fix ist\n",
        "\n",
        "planner_chain = planner_prompt | model.with_structured_output(Plan)\n",
        "print(\"OK – Planner-Chain gebaut.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1AZtvQIwR3o1",
        "outputId": "81bd4a0c-ab6c-43ff-a2bc-c2c5838d20cc"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OK – Planner-Chain gebaut.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Actor/Agent (Tool-Calling, Schritt 2)"
      ],
      "metadata": {
        "id": "26MhxzVsR8FV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== 3) ACTOR (Agent mit Tools) =====\n",
        "# Der Actor ist die zweite Stufe unserer Agenten-Pipeline.\n",
        "# Er übernimmt den vom Planner erstellten Plan und führt die nötigen Schritte mit den registrierten Tools aus.\n",
        "#\n",
        "# 1. Prompt-Template `agent_prompt`\n",
        "# ---------------------------------\n",
        "# - Systemrolle: Weist das Modell an, faktenbasiert zu antworten und Tools nur dann zu nutzen, wenn sie wirklich gebraucht werden.\n",
        "#   Außerdem soll der \"scratchpad\"-Bereich (der interne Gedankenbereich des Agents) maximal 30 Wörter enthalten,\n",
        "#   um Tokenverbrauch zu reduzieren.\n",
        "# - Humanrolle: Platzhalter {input} – hier wird die eigentliche Anweisung oder Teilaufgabe des Agents eingefügt.\n",
        "# - Placeholder `{agent_scratchpad}`: Dieser spezielle Platzhalter wird von LangChain intern genutzt,\n",
        "#   um den aktuellen Stand der Zwischenergebnisse und Tool-Calls einzufügen.\n",
        "#\n",
        "# Vorteil:\n",
        "# - Klare Trennung zwischen Nutzerinput und internen Agenten-Notizen\n",
        "# - Minimierung des Tokenverbrauchs durch bewusst knappe Scratchpads\n",
        "\n",
        "agent_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\",\n",
        "     \"Antworte faktenbasiert. Nutze Tools nur falls nötig. \"\n",
        "     \"Halte dein scratchpad extrem kurz (max. 30 Wörter).\"),\n",
        "    (\"human\", \"{input}\"),\n",
        "    (\"placeholder\", \"{agent_scratchpad}\")\n",
        "])\n",
        "\n",
        "\n",
        "# 2. AgentExecutor `agent_exec`\n",
        "# -----------------------------\n",
        "# - `agent`: Der Agent selbst (wird vorher mit `create_openai_tools_agent(...)` gebaut)\n",
        "# - `tools`: Liste der verfügbaren Tools (z. B. `rag_search`, `calc`)\n",
        "# - `verbose=False`: Deaktiviert die ausführliche Konsolenausgabe für weniger Clutter\n",
        "# - `max_iterations=2`: Beschränkt den Agenten auf maximal 2 Schritte, um unnötige Tool-Schleifen zu vermeiden\n",
        "# - `early_stopping_method=\"generate\"`: Falls das Limit erreicht ist, erzeugt der Agent trotzdem eine Antwort\n",
        "#\n",
        "# Vorteil:\n",
        "# - Verhindert Endlosschleifen bei Tool-Aufrufen\n",
        "# - Spart Tokens, weil unnötige Iterationen und zu lange Scratchpads vermieden werden\n",
        "\n",
        "agent_exec = AgentExecutor(\n",
        "    agent=agent,\n",
        "    tools=tools,\n",
        "    verbose=False,\n",
        "    max_iterations=2,\n",
        "    early_stopping_method=\"generate\"\n",
        ")\n",
        "\n",
        "\n",
        "print(\"OK – AgentExecutor bereit.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ukl0uS1ZR_Xc",
        "outputId": "aa6bbd88-4513-4717-97c3-4d6c76f62ee7"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OK – AgentExecutor bereit.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Formatter (Structured Output, Schritt 3)"
      ],
      "metadata": {
        "id": "Hf78duDwSDRt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== 4) FORMATTER (Structured Output) =====\n",
        "# Der Formatter ist die letzte Stufe in der Agenten-Pipeline.\n",
        "# Er nimmt alle Zwischenergebnisse (Plan + Agenten-Trace) und wandelt sie in ein\n",
        "# strukturiertes, einheitliches Antwortformat um, das leicht weiterverarbeitet werden kann.\n",
        "#\n",
        "# 1. Datenmodelle (Pydantic)\n",
        "# --------------------------\n",
        "# - `TableRow`: Einfaches Modell mit zwei Strings (key, value) für tabellarische Zusatzinfos.\n",
        "# - `FinalAnswer`: Enthält alle Bestandteile der finalen Antwort:\n",
        "#     - answer: Fließtext, kurz und präzise\n",
        "#     - bullet_points: Liste der wichtigsten Erkenntnisse\n",
        "#     - citations: Quellenangaben im Format [1], [2] → Verweis auf RAG-Snippets\n",
        "#     - table: Liste von `TableRow`-Objekten, falls strukturierte Daten enthalten sind\n",
        "#     - steps_taken: Liste kurzer Protokollpunkte zu den Schritten, die der Agent gemacht hat\n",
        "#\n",
        "# Vorteil:\n",
        "# - Einheitliches Output-Format → keine Nachbearbeitung der rohen LLM-Antwort nötig\n",
        "# - Struktur erleichtert maschinelle Weiterverarbeitung (Export, Anzeige, API-Response)\n",
        "\n",
        "\n",
        "class TableRow(BaseModel):\n",
        "    key: str\n",
        "    value: str\n",
        "\n",
        "class FinalAnswer(BaseModel):\n",
        "    answer: str = Field(..., description=\"Kompakte, präzise Antwort in Fließtext.\")\n",
        "    bullet_points: List[str] = Field(default_factory=list, description=\"Wichtige Takeaways.\")\n",
        "    citations: List[str] = Field(default_factory=list, description=\"Referenzen wie [1], [2] die auf RAG-Snippets verweisen.\")\n",
        "    table: List[TableRow] = Field(default_factory=list, description=\"Optional: kleine Tabelle mit Kernwerten.\")\n",
        "    steps_taken: List[str] = Field(default_factory=list, description=\"Kurzprotokoll der Agentenschritte.\")\n",
        "\n",
        "\n",
        "# 2. Prompt-Template `formatter_prompt`\n",
        "# -------------------------------------\n",
        "# - Systemrolle: Weist das Modell an, die Antwort kompakt zu halten\n",
        "#   (max. 80 Wörter in `answer`, max. 4 Bullet Points, max. 2 Quellen, max. 3 Tabellenzeilen).\n",
        "#   Falls die Faktenlage unklar ist, soll das Modell explizit \"Ich weiß es nicht.\" schreiben.\n",
        "# - Humanrolle: Übermittelt die Eingabedaten für den Formatter:\n",
        "#   - {question} → die Originalfrage\n",
        "#   - {plan} → der vom Planner erstellte Kurzplan\n",
        "#   - {trace} → gekürzte Agenten-Notizen inkl. Tool-Ausgaben\n",
        "#\n",
        "# Vorteil:\n",
        "# - Garantierte Begrenzung der Antwortlänge (spart Tokens)\n",
        "formatter_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\",\n",
        "     \"Formatiere kompakt ins Schema. \"\n",
        "     \"answer ≤ 80 Wörter, ≤ 4 bullets, ≤ 2 citations, ≤ 3 table rows. \"\n",
        "     \"Wenn Evidenz fehlt: 'Ich weiß es nicht.'\"),\n",
        "    (\"human\",\n",
        "     \"Frage: {question}\\nPlan(kurz): {plan}\\nTrace(kurz):\\n{trace}\\nErzeuge das Ergebnis.\")\n",
        "])\n",
        "\n",
        "# 3. Verkettung mit Modell (`formatter_chain`)\n",
        "# --------------------------------------------\n",
        "# - Das Prompt-Template wird mit `model.with_structured_output(FinalAnswer)` kombiniert.\n",
        "# - Das sorgt dafür, dass das Modell direkt ein `FinalAnswer`-Objekt ausgibt, ohne dass man\n",
        "#   selbst JSON-Parsing und Validierung implementieren muss.\n",
        "#\n",
        "# Vorteil:\n",
        "# - Kein manuelles Parsen oder Validieren nötig\n",
        "# - Output kommt garantiert im richtigen Format zurück\n",
        "# WICHTIG: Prompt VOR Modell verkettet, dann structured output\n",
        "formatter_chain = formatter_prompt | model.with_structured_output(FinalAnswer)\n",
        "print(\"OK – Formatter-Chain gebaut.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nlVUVLXZSFxV",
        "outputId": "fb106106-22a0-49f3-f89f-396120584397"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OK – Formatter-Chain gebaut.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Orchestrator (Planner → Actor → Formatter)"
      ],
      "metadata": {
        "id": "NOeSKsxtSLNN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== ORCHESTRATOR: answer_with_agents =====\n",
        "# Diese Funktion ist das zentrale Bindeglied zwischen allen Komponenten (Planner, Actor, Formatter).\n",
        "# Sie koordiniert die komplette Pipeline, um aus einer Nutzerfrage eine strukturierte Antwort zu erzeugen.\n",
        "#\n",
        "# Ablauf:\n",
        "# -------\n",
        "# 1. Eingabe:\n",
        "#    - `question`: Die Nutzerfrage als String.\n",
        "#\n",
        "# 2. Plan erstellen (Planner-Phase):\n",
        "#    - Ruft `planner_chain.invoke(...)` auf, um aus der Frage einen strukturierten Plan\n",
        "#      (Pydantic-Objekt vom Typ `Plan`) zu generieren.\n",
        "#    - Dieser Plan enthält Flags, ob RAG-Suche oder Berechnung nötig ist, sowie die konkreten Queries/Expressions.\n",
        "#\n",
        "# 3. Zwischenspeicher (trace_parts):\n",
        "#    - Initialisiert eine Liste `trace_parts`, in der alle Zwischenschritte protokolliert werden.\n",
        "#    - Fügt zuerst den erzeugten Plan in gekürzter Form (via `_clip`) ein, um Tokenverbrauch zu reduzieren.\n",
        "#\n",
        "# 4. Kontext-Retrieval (RAG):\n",
        "#    - Prüft, ob im Plan `needs_retrieval=True` steht und ob `queries` vorhanden sind.\n",
        "#    - Falls ja: Führt nur die erste Query (`[:1]`) mit `agent_exec.invoke(...)` aus.\n",
        "#      → Dadurch wird der Actor-Block aufgerufen, der das Tool `rag_search` benutzt.\n",
        "#    - Fügt das (ggf. gekürzte) Ergebnis in den Trace ein.\n",
        "#    - Falls keine RAG-Suche nötig ist, wird \"(übersprungen)\" protokolliert.\n",
        "#\n",
        "# 5. Berechnungen (CALC):\n",
        "#    - Prüft, ob im Plan `needs_calculation=True` steht und ob `calc_expressions` vorhanden sind.\n",
        "#    - Falls ja: Führt nur den ersten Ausdruck (`[:1]`) mit `agent_exec.invoke(...)` aus.\n",
        "#      → Hier ruft der Actor das Tool `calc` auf.\n",
        "#    - Fügt das (ggf. gekürzte) Ergebnis in den Trace ein.\n",
        "#    - Falls keine Berechnung nötig ist, wird \"(übersprungen)\" protokolliert.\n",
        "#\n",
        "# 6. Trace zusammenfassen:\n",
        "#    - Verbindet alle gesammelten Trace-Teile (`trace_parts`) zu einem String.\n",
        "#    - Kürzt diesen String nochmals mit `_clip(..., 1400)`, um die Tokenmenge im Formatter-Aufruf gering zu halten.\n",
        "#\n",
        "# 7. Formatierung (Formatter-Phase):\n",
        "#    - Ruft `formatter_chain.invoke(...)` auf und übergibt:\n",
        "#        * Die Originalfrage\n",
        "#        * Den Plan als Dictionary (`plan.model_dump()`)\n",
        "#        * Den gekürzten Trace\n",
        "#    - Ergebnis: Ein `FinalAnswer`-Objekt mit strukturierter, kompakter Antwort.\n",
        "#\n",
        "# Vorteil:\n",
        "# - Kontrollierte Token-Nutzung durch gezieltes Kürzen (`_clip`)\n",
        "# - Klare Trennung von Planung, Ausführung und Formatierung\n",
        "# - Jeder Schritt ist einzeln austauschbar oder erweiterbar\n",
        "#\n",
        "# Sicherheitsmechanismen:\n",
        "# - Maximal 1 Query und 1 Berechnung pro Anfrage (verhindert unkontrolliertes Wachstum)\n",
        "# - Kürzungen bei Plan-, RAG- und CALC-Ausgaben\n",
        "# - Gekappter Gesamtspeicher (`trace`) für den Formatter\n",
        "\n",
        "def answer_with_agents(question: str) -> FinalAnswer:\n",
        "    # Hilfsfunktion: Kürzt Strings auf n Zeichen und hängt \"…\" an, falls nötig\n",
        "    def _clip(s, n): return (s[:n] + \"…\") if len(s) > n else s\n",
        "\n",
        "    # 1) Plan erstellen\n",
        "    plan: Plan = planner_chain.invoke({\"question\": question})\n",
        "\n",
        "    # 2) Trace initialisieren\n",
        "    trace_parts = []\n",
        "    trace_parts.append(f\"# Plan\\n{_clip(plan.model_dump_json(indent=0), 500)}\")\n",
        "\n",
        "    # 3) Kontext-Retrieval (RAG)\n",
        "    if plan.needs_retrieval and plan.queries:\n",
        "        for q in plan.queries[:1]:  # nur die erste Query nutzen\n",
        "            out = agent_exec.invoke({\"input\": f\"Rufe rag_search auf für Query: {q}\"})\n",
        "            trace_parts.append(f\"\\n# RAG ({q})\\n{_clip(out.get('output',''), 500)}\")\n",
        "    else:\n",
        "        trace_parts.append(\"\\n# RAG\\n(übersprungen)\")\n",
        "\n",
        "    # 4) Berechnung (CALC)\n",
        "    if plan.needs_calculation and plan.calc_expressions:\n",
        "        for expr in plan.calc_expressions[:1]:  # nur den ersten Ausdruck nutzen\n",
        "            out = agent_exec.invoke({\"input\": f\"Berechne: {expr} (nutze calc)\"})\n",
        "            trace_parts.append(f\"\\n# CALC ({expr})\\n{_clip(out.get('output',''), 150)}\")\n",
        "    else:\n",
        "        trace_parts.append(\"\\n# CALC\\n(übersprungen)\")\n",
        "\n",
        "    # 5) Trace zusammenfassen und kürzen\n",
        "    trace = _clip(\"\\n\".join(trace_parts), 1400)\n",
        "\n",
        "    # 6) Formatierte Endausgabe erstellen\n",
        "    final: FinalAnswer = formatter_chain.invoke({\n",
        "        \"question\": question,\n",
        "        \"plan\": plan.model_dump(),\n",
        "        \"trace\": trace\n",
        "    })\n",
        "    return final\n"
      ],
      "metadata": {
        "id": "qTHDsfJpSK51"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Beispiele (laufen lassen & JSON ausgeben)"
      ],
      "metadata": {
        "id": "8Mf5tYXdSRtP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Beispiel 1: Reine inhaltliche Frage → nutzt RAG\n",
        "final1 = answer_with_agents(\n",
        "    \"Erkläre kurz die Vorteile von BTC-Mining in Verbindung mit Abwärmenutzung in Gewächshäusern.\"\n",
        ")\n",
        "print(final1.model_dump_json(indent=2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lxj-NVclUtdA",
        "outputId": "585338e2-7696-485d-95de-ef3f082ca357"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"answer\": \"BTC-Mining in Verbindung mit Abwärmenutzung in Gewächshäusern bietet mehrere Vorteile: Es schafft eine zusätzliche Einkommensquelle für Landwirte, indem Abwärme zur Beheizung der Gewächshäuser genutzt wird. Dies verbessert die Energieeffizienz und reduziert Betriebskosten. Zudem fördert es die Nachhaltigkeit, indem Abwärme, die sonst verloren ginge, sinnvoll eingesetzt wird. Die Synergie zwischen beiden Bereichen kann die Rentabilität steigern und die Abhängigkeit von fossilen Brennstoffen verringern.\",\n",
            "  \"bullet_points\": [\n",
            "    \"Zusätzliche Einkommensquelle für Landwirte\",\n",
            "    \"Energieeffizienz durch Abwärmenutzung\",\n",
            "    \"Reduzierung der Betriebskosten\",\n",
            "    \"Nachhaltigkeit und Ressourcenschonung\"\n",
            "  ],\n",
            "  \"citations\": [\n",
            "    \"[1]\",\n",
            "    \"[2]\"\n",
            "  ],\n",
            "  \"table\": [\n",
            "    {\n",
            "      \"key\": \"Einkommensquelle\",\n",
            "      \"value\": \"Zusätzliches Einkommen durch Mining\"\n",
            "    },\n",
            "    {\n",
            "      \"key\": \"Energieeffizienz\",\n",
            "      \"value\": \"Nutzung von Abwärme zur Beheizung\"\n",
            "    },\n",
            "    {\n",
            "      \"key\": \"Nachhaltigkeit\",\n",
            "      \"value\": \"Reduzierung fossiler Brennstoffe\"\n",
            "    }\n",
            "  ],\n",
            "  \"steps_taken\": [\n",
            "    \"Vorteile von BTC-Mining identifiziert\",\n",
            "    \"Abwärmenutzung in Gewächshäusern erklärt\",\n",
            "    \"Synergien zwischen beiden Konzepten herausgearbeitet\"\n",
            "  ]\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Beispiel 2: Mit kleiner Rechnung (Tool calc) → Prompt-Chaining + Tools\n",
        "final2 = answer_with_agents(\n",
        "    \"Wenn ein Miner 3 kW Abwärme liefert und ein Gewächshaus 9 kW Heizbedarf hat, \"\n",
        "    \"wie viele Miner wären nötig? Erkläre kurz und nutze die PDF als Kontext.\"\n",
        ")\n",
        "print(final2.model_dump_json(indent=2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kF1g7PD0SSem",
        "outputId": "8bec2076-253c-4f56-8fa9-f45a9fafbd1e"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"answer\": \"Um den Heizbedarf eines Gewächshauses von 9 kW zu decken, sind 3 Miner erforderlich, da jeder Miner 3 kW Abwärme liefert. Die Berechnung lautet: 9 kW / 3 kW = 3 Miner.\",\n",
            "  \"bullet_points\": [\n",
            "    \"Heizbedarf Gewächshaus: 9 kW\",\n",
            "    \"Abwärme pro Miner: 3 kW\",\n",
            "    \"Benötigte Miner: 3\",\n",
            "    \"Effiziente Nutzung von Abwärme.\"\n",
            "  ],\n",
            "  \"citations\": [\n",
            "    \"1\",\n",
            "    \"2\"\n",
            "  ],\n",
            "  \"table\": [\n",
            "    {\n",
            "      \"key\": \"Heizbedarf Gewächshaus\",\n",
            "      \"value\": \"9 kW\"\n",
            "    },\n",
            "    {\n",
            "      \"key\": \"Abwärme pro Miner\",\n",
            "      \"value\": \"3 kW\"\n",
            "    },\n",
            "    {\n",
            "      \"key\": \"Benötigte Miner\",\n",
            "      \"value\": \"3\"\n",
            "    }\n",
            "  ],\n",
            "  \"steps_taken\": [\n",
            "    \"Heizbedarf des Gewächshauses bestimmt.\",\n",
            "    \"Abwärme eines einzelnen Miners ermittelt.\",\n",
            "    \"Anzahl der benötigten Miner berechnet.\"\n",
            "  ]\n",
            "}\n"
          ]
        }
      ]
    }
  ]
}